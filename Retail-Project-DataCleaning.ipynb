{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8d04720-7cbb-4ad9-a81a-63deccf8c029",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_df = spark.read.csv(\"/Volumes/sample_catalog/default/db_catalog/sales_data.csv\", header=True, inferSchema=True)\n",
    "store_df = spark.read.csv(\"/Volumes/sample_catalog/default/db_catalog/store_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "display(sales_df)\n",
    "display(store_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4e57368-9ce6-4530-9ecf-2055c79831c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data clean-up for sales data\n",
    "\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "sales_df = sales_df.fillna({\"quantity\": 0, \"total_amount\": 0})\n",
    "sales_df = sales_df.dropDuplicates()\n",
    "sales_df = sales_df.filter(col(\"sale_id\").isNotNull())\n",
    "sales_df = sales_df.filter(col(\"store_id\").isNotNull())\n",
    "sales_df = sales_df.filter(col(\"sale_date\").isNotNull())\n",
    "sales_df = sales_df.filter((col(\"quantity\") > 0) & (col(\"total_amount\") > 0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display(sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8a0facb-177a-4ae8-baec-e640ff306a7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data clean-up for store data\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "store_df = store_df.filter(col(\"store_id\").isNotNull())\n",
    "avg_store_size = store_df.select(avg(\"store_size\")).first()[0]\n",
    "print(avg_store_size)\n",
    "store_df = store_df.fillna({\"store_size\": avg_store_size})\n",
    "store_df = store_df.filter(col(\"open_date\").isNotNull())\n",
    "\n",
    "display(store_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c11b6afc-a1a3-4a64-840d-42cd021b582a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data transformation between sales and store data\n",
    "\n",
    "combined_df = sales_df.join(store_df, on = \"store_id\", how = \"inner\")\n",
    "display(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88344863-60c6-48de-9bbf-74afc3b81565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data aggregation between sales and store data\n",
    "\n",
    "from pyspark.sql.functions import year, col, round\n",
    "\n",
    "combined_df = combined_df.withColumn(\"year\", year(col(\"sale_date\")))\n",
    "combined_df = combined_df.withColumn(\"sale_per_sqft\", round(col(\"total_amount\") / col(\"store_size\"), 2))\n",
    "\n",
    "display(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b3f5e5a-a653-4b8a-95cb-fae46e76243f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"total_sales\":179},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769697959760}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a viwew of the combined data to calculate the total sales and total quantity per store and region \n",
    "\n",
    "combined_df.createOrReplaceTempView(\"combined_v\")\n",
    "store_sales_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        store_id, \n",
    "        store_region, \n",
    "        SUM(total_amount) as total_sales, \n",
    "        SUM(quantity) as total_quantity \n",
    "    FROM combined_v\n",
    "    GROUP BY store_id, store_region\n",
    "\"\"\")\n",
    "\n",
    "display(store_sales_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1262c24-02a3-4ca8-a363-10e9c0a9cbdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find out top 5 products by toital quantity sold\n",
    "\n",
    "top_products_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        product_id, \n",
    "        SUM(quantity) AS total_quantity \n",
    "    FROM combined_v\n",
    "    GROUP BY product_id\n",
    "    ORDER BY total_quantity DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "display(top_products_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1882b2f-5867-409c-a9ce-b9e3d5cc2002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find out top 5 stores by quantity sold\n",
    "\n",
    "store_sales_sql.createOrReplaceTempView(\"store_sales\")\n",
    "top_store = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        store_id, \n",
    "        total_sales \n",
    "    FROM store_sales\n",
    "    ORDER BY total_sales DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "display(top_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f12c96-f7e4-43e1-aa0f-7b815e4c4a11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the output results as a parquet file to the volume or path\n",
    "\n",
    "top_products_sql.write.parquet(\"/Volumes/sample_catalog/default/db_catalog/top_products\")\n",
    "top_store.write.parquet(\"/Volumes/sample_catalog/default/db_catalog/top_store\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Retail-Project-DataCleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
